{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recruitment Analysis\n",
    "\n",
    "A simple script to test for anomalies within recruitment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.regressionplots import abline_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the recruitment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitment = pd.read_csv(\"recruitment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for those who are unable to commit 1 year to PV\n",
    "\n",
    "\n",
    "The idea is simple, just checked who did not tick yes to the commitment question in the google form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we aim to clean the table to remove unnecessary columns for easier viewing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unnecessary columns such as tag number, year of study, graduating\n",
    "del recruitment['Tag Number']\n",
    "del recruitment['1h) As of August 2021, what year of study would you be in?']\n",
    "del recruitment['1i) Will you be graduating before Jun 2022?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We check for those that have not checked for the commitments section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let me check what is the data type for the column\n",
    "recruitment['1j) Should you be accepted into Protege Ventures, would you be able to commit to sessions on a weekly basis for the next 12 months? Sessions will be suspended during exam season.'].dtype\n",
    "\n",
    "# so it is a string basically, okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unavailable = recruitment.loc[recruitment['1j) Should you be accepted into Protege Ventures, would you be able to commit to sessions on a weekly basis for the next 12 months? Sessions will be suspended during exam season.'] != 'checked']\n",
    "len(unavailable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since everyone can commit to PV, seeing that there are 0 people who unchecked, this column is also unnecessary, and can be removed to save columns, together with the next column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del recruitment['1j) Should you be accepted into Protege Ventures, would you be able to commit to sessions on a weekly basis for the next 12 months? Sessions will be suspended during exam season.']\n",
    "del recruitment['1k) What other commitments might you have for the next 12 months? (ie CCA Clubs, Freelance work, etc)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Testing for lenient assessors\n",
    "\n",
    "This idea revolves around generating average word count for each score, for each question.\n",
    "\n",
    "I.e. for question 1, average word count for score 1, 2, 3, 4, 5 is maybe 50, 100, 150, 200, 250, through counting each word in the answer, and plotting a regression. We continue this for each question.\n",
    "\n",
    "We can see thus who tends to grade above / below the mean scores for several questions, and flag them out for being lenient.\n",
    "\n",
    "We would also need to run a hypothesis test to prove that this is the case actually, that we can in fact, use word count as a way to determine potential score one should received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to clean up the table, so let's check what the type of the scores is, and change it to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a random variable\n",
    "variable = recruitment.iloc[0, [3]]\n",
    "variable.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we clean for all the columns that have scores present in them\n",
    "\n",
    "# firstly we start by creating a column list with the relevant numbers needed\n",
    "column_list = [3, 4, 6, 7, 9, 10, 12, 13, 21, 22, 24, 25]\n",
    "\n",
    "# now loop through the column:\n",
    "for index in column_list:\n",
    "    \n",
    "    # loop through each cell in each column\n",
    "    for i, row_value in recruitment.iloc[:, [index]].iterrows():\n",
    "        \n",
    "        # change the values to strings so that you can remove extra comments\n",
    "        recruitment.iloc[i, [index]] = recruitment.iloc[i, [index]].astype(str)\n",
    "        \n",
    "        # now remove any newlines from the cells\n",
    "        recruitment.iloc[i, [index]] = recruitment.iloc[i, [index]].replace('\\n','', regex=True)\n",
    "        \n",
    "        # because some scores have ranknigs attached to them, i.e. applicant 67: 2: I don't see much .... score\n",
    "        # so we have to remove this, by taking the first score attached, i.e. take 2 and filter out the rest\n",
    "        recruitment.iloc[i, [index]] = recruitment.iloc[i, [index]].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the strings into integers because it doesn't work by iloc somehow    \n",
    "recruitment[recruitment.columns[column_list]] = recruitment[recruitment.columns[column_list]].apply(pd.to_numeric, errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we try to split up each part, and create tables to store the text count for each answer of each applicant, for each question in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [3, 4, 6, 7, 9, 10, 12, 13, 21, 22, 24, 25]\n",
    "\n",
    "# creating the various tables for the various questions\n",
    "question_one = recruitment.iloc[:, [0, 1, 2, 3, 4]].copy()\n",
    "question_two = recruitment.iloc[:, [0, 1, 5, 6, 7]].copy()\n",
    "question_three = recruitment.iloc[:, [0, 1, 8, 9, 10]].copy()\n",
    "question_four = recruitment.iloc[:, [0, 1, 11, 12, 13]].copy()\n",
    "question_five = recruitment.iloc[:, [0, 1, 20, 21, 22]].copy()\n",
    "question_six = recruitment.iloc[:, [0, 1, 23, 24, 25]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import wordcount\n",
    "\n",
    "# create a column for the word count\n",
    "question_one['WordCount'] = np.nan\n",
    "\n",
    "wordcount(question_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Data Cleaning! Change all assessor scores that are 0 to 1:\n",
    "\n",
    "Because apparently some people didn't read the question. It states rank from 1 - 5, not 0 - 5. So 0 and 1 scores have the same weight basically, we can just change 0 scores to 1 for those that actually read the questions properly.\n",
    "\n",
    "This will mess up the standard deviations later if not done so, so let us change the scores that are 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import cleanscores\n",
    "\n",
    "# create a loop to look through all the assessor scores and change it to 1\n",
    "weighted_score = []\n",
    "\n",
    "cleanscores(question_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's get the average scores of each applicant, by mean of both assessors:\n",
    "\n",
    "This is important because as tested later, it is ideal that we find a variable for writing ability. Some people may write abit, but the content is lacking in actual material or quality. We need to find a variable for writing ability. \n",
    "\n",
    "Writing ability can be quantified by experience / ability to convey experience. And we can use experience in question 1I as a proxy variable to reduce this omitted variable bias, to reduce wild standard deviations in the tests later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the average score of each applicant\n",
    "question_one['AverageScore'] = (question_one[\"1l Assesor A's Score (1-5)\"] + question_one[\"1l Assessor B's Score (1-5)\"]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we need to clean up the table even more, by merging assessor A and B into the same column:\n",
    "\n",
    "The idea is to do so through copying the table into another table, removing assessor A and their scores for the first table, and then removing assessor B and their scores for the second table, and then placing assessor B table under assessor A table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get assessor A scores for the first table, aka drop assessor b scores\n",
    "question_one_a = question_one.copy()\n",
    "\n",
    "# get assessor B scores for the second table, aka drop assessor a scores\n",
    "question_one_b = question_one.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to drop columns of assessor B and their scores in the first table\n",
    "del question_one_a['Assesor B']\n",
    "del question_one_a[\"1l Assessor B's Score (1-5)\"]\n",
    "\n",
    "# and we also need to drop columns of assessor A and their scores in the second table\n",
    "del question_one_b['Assesor A']\n",
    "del question_one_b[\"1l Assesor A's Score (1-5)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now change the colmnn names of table one\n",
    "question_one_a = question_one_a.rename(columns = {\"Assesor A\": \"Assessor\", \"1l Assesor A's Score (1-5)\": \"AssessorScore\"})\n",
    "\n",
    "# and let's also change the column names of table two so that we can merge the tables together\n",
    "question_one_b = question_one_b.rename(columns = {\"Assesor B\": \"Assessor\", \"1l Assessor B's Score (1-5)\": \"AssessorScore\"})\n",
    "\n",
    "# now concatenate the two values together into a table containing everybody's scores for question one\n",
    "question_one = pd.concat([question_one_a, question_one_b], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import logcount\n",
    "\n",
    "# get the question score and word count table\n",
    "q_one_scores = question_one.iloc[:, [2, 3, 4]].copy()\n",
    "\n",
    "# add logarithmic count to the word count\n",
    "logcount(q_one_scores)\n",
    "\n",
    "q_one_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a linear regression of log(word count) against the ordinal scores:\n",
    "\n",
    "Initially, wanted to run ordinal regression. On second thought, it is not necessary because scores can be quantified, and differences can also be quantified. A linear / logistic regression works fine in this case.\n",
    "\n",
    "The current equation works as follows:\n",
    "\n",
    "$ {score} = {\\beta}_{0} + {\\beta}_{j} \\:wordcount_{j} + Îµ $\n",
    "\n",
    "Currently the understanding for the linear regression is that:\n",
    "\n",
    "$\\hat{score} = \\hat{\\beta}_{0} + \\hat{\\beta}_{j} \\:log(wordcount_{j}) $\n",
    "\n",
    "Of course, there is the case whereby we fail to account for a variable (omitted variable bias) of experience, as well as language ability.\n",
    "\n",
    "A proxy for experience would be question one itself, asking about experience, for the next questions. As established earlier, to combat this omitted variable bias, we should find other possible proxy variables. Due to the lack of such a variable, we can't adopt this solution.\n",
    "\n",
    "$\\hat{score} = \\hat{\\beta}_{0} + \\hat{\\beta}_{j} \\:log(wordcount _{j}) + \\hat{\\beta}_{k} \\:proxy _{k} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the linear regression library from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from leniency import regressquestion\n",
    "\n",
    "# creating variables here for easier use later\n",
    "X = q_one_scores['LogCount'].values.reshape(-1, 1)\n",
    "Y = q_one_scores['AssessorScore'].values.reshape(-1, 1)\n",
    "\n",
    "regressquestion(q_one_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to get the std err variable for our calculations later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,0], Y[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now getting a summary of the entire statistics of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import regressionstats\n",
    "\n",
    "# find the regression statistics\n",
    "regressionstats(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we have two hypothesis, and need to test if the hypothesis is true:\n",
    "\n",
    "${H_{0}} : \\hat{\\beta_{j}} = 0 $ , word count does not affect scores  \n",
    "${H_{1}} : \\hat{\\beta_{j}} > 0 $ , word count affects scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a significance level of $ {\\alpha} = 0.01 $\n",
    "\n",
    "The coefficient of log(word count) is significantly smaller than the significance level as seen from P > t, hence word count thus affect the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus equation is:\n",
    "\n",
    "$\\hat{score} = \\hat{1.079} + \\hat{0.7025} \\:log(wordcount_{j}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now find the predicted scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import predictedscore\n",
    "\n",
    "# find the predicted score now\n",
    "predictedscore(question_one, slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can try using a weighted predicted score using the average scores as a baseline for accuracy:\n",
    "\n",
    "To reduce the chance of shitty answers being over-predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from leniency import weightedscore\n",
    "\n",
    "# now to get the weighted score\n",
    "weightedscore(question_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second algorithm for coming up with a weighted predicted score that punishes bad scores less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import adjustedscore\n",
    "\n",
    "# now to get the adjusted weighted score\n",
    "adjustedscore(question_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to add the standard errors away from the assessor score, and also include average score:\n",
    "\n",
    "The point of the average scores is that, maybe both assessors think the applicant answer is shit, but apparently it got a good grading on this model because of the false assumption of word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leniency import assessordeviation\n",
    "\n",
    "# now to get the standard deviations of the individual assessor\n",
    "assessordeviation(question_one, std_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classification\n",
    "\n",
    "We can now collect the standard deviations and find averages for each person, to test their leniency / strictness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from leniency import namedict\n",
    "\n",
    "# now create a dictionary to store all the names and their relevant score for the particular qn\n",
    "\n",
    "# after storing all the names, immediately add it to the deviation table\n",
    "deviation_table = pd.DataFrame([namedict(question_one)])\n",
    "deviation_table = deviation_table.transpose().reset_index()\n",
    "deviation_table.columns = ['Assessor', 'q1i']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's fill up the rest for the other questions that we have:\n",
    "\n",
    "We are just copying the same exact script for the other 5 questions so let's just simplify the entire process and remove line breaks, comments, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating for all the questions' table\n",
    "question_two['WordCount'] = np.nan\n",
    "wordcount(question_two)\n",
    "    \n",
    "question_three['WordCount'] = np.nan\n",
    "wordcount(question_three)\n",
    "\n",
    "question_four['WordCount'] = np.nan\n",
    "wordcount(question_four)\n",
    "    \n",
    "question_five['WordCount'] = np.nan\n",
    "wordcount(question_five)\n",
    "\n",
    "question_six['WordCount'] = np.nan\n",
    "wordcount(question_six)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data for all the questions' table\n",
    "weighted_score = []\n",
    "cleanscores(question_two)\n",
    "\n",
    "weighted_score = []\n",
    "cleanscores(question_three)\n",
    "\n",
    "weighted_score = []\n",
    "cleanscores(question_four)\n",
    "\n",
    "weighted_score = []\n",
    "cleanscores(question_five)\n",
    "\n",
    "weighted_score = []\n",
    "cleanscores(question_six)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the average score for each table\n",
    "question_two['AverageScore'] = (question_two[\"2a Assesor A's Score (1-5)\"] + question_two[\"2a Assesor B's Score (1-5)\"]) / 2\n",
    "\n",
    "question_three['AverageScore'] = (question_three[\"2b Assesor A's Score (1-5)\"] + question_three[\"2b Assessor B's Score (1-5)\"]) / 2\n",
    "\n",
    "question_four['AverageScore'] = (question_four[\"2c Assesor A's Score (1-5)\"] + question_four[\"2c Assessor B's Score (1-5)\"]) / 2\n",
    "\n",
    "question_five['AverageScore'] = (question_five[\"2f Assesor A's Score (1-5)\"] + question_five[\"2f Assessor B's Score (1-5)\"]) / 2\n",
    "\n",
    "question_six['AverageScore'] = (question_six[\"2g Assesor A's Score (1-5)\"] + question_six[\"2g Assessor B's Score (1-5)\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary transformations for table 2\n",
    "question_two_a = question_two.copy()\n",
    "question_two_b = question_two.copy()\n",
    "\n",
    "del question_two_a['Assesor B']\n",
    "del question_two_a[\"2a Assesor B's Score (1-5)\"]\n",
    "del question_two_b['Assesor A']\n",
    "del question_two_b[\"2a Assesor A's Score (1-5)\"]\n",
    "\n",
    "question_two_a = question_two_a.rename(columns = {\"Assesor A\": \"Assessor\", \"2a Assesor A's Score (1-5)\": \"AssessorScore\"})\n",
    "question_two_b = question_two_b.rename(columns = {\"Assesor B\": \"Assessor\", \"2a Assesor B's Score (1-5)\": \"AssessorScore\"})\n",
    "question_two = pd.concat([question_two_a, question_two_b], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_two_scores = question_two.iloc[:, [2, 3, 4]].copy()\n",
    "logcount(q_two_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q_two_scores['LogCount'].values.reshape(-1, 1)\n",
    "Y = q_two_scores['AssessorScore'].values.reshape(-1, 1)\n",
    "regressquestion(q_two_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionstats(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,0], Y[:,0])\n",
    "\n",
    "predictedscore(question_two, slope, intercept)\n",
    "weightedscore(question_two)\n",
    "adjustedscore(question_two)\n",
    "assessordeviation(question_two, std_err)\n",
    "\n",
    "deviation_table['q2a'] = deviation_table['Assessor'].map(namedict(question_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue for Question 3 Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary transformations for table 3\n",
    "question_three_a = question_three.copy()\n",
    "question_three_b = question_three.copy()\n",
    "\n",
    "del question_three_a['Assesor B']\n",
    "del question_three_a[\"2b Assessor B's Score (1-5)\"]\n",
    "del question_three_b['Assesor A']\n",
    "del question_three_b[\"2b Assesor A's Score (1-5)\"]\n",
    "\n",
    "question_three_a = question_three_a.rename(columns = {\"Assesor A\": \"Assessor\", \"2b Assesor A's Score (1-5)\": \"AssessorScore\"})\n",
    "question_three_b = question_three_b.rename(columns = {\"Assesor B\": \"Assessor\", \"2b Assessor B's Score (1-5)\": \"AssessorScore\"})\n",
    "question_three = pd.concat([question_three_a, question_three_b], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_three_scores = question_three.iloc[:, [2, 3, 4]].copy()\n",
    "logcount(q_three_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_three_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q_three_scores['LogCount'].values.reshape(-1, 1)\n",
    "Y = q_three_scores['AssessorScore'].values.reshape(-1, 1)\n",
    "regressquestion(q_three_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionstats(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,0], Y[:,0])\n",
    "\n",
    "predictedscore(question_three, slope, intercept)\n",
    "weightedscore(question_three)\n",
    "adjustedscore(question_three)\n",
    "assessordeviation(question_three, std_err)\n",
    "\n",
    "deviation_table['q2b'] = deviation_table['Assessor'].map(namedict(question_three))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue for Question 4 Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary transformations for table 4\n",
    "question_four_a = question_four.copy()\n",
    "question_four_b = question_four.copy()\n",
    "\n",
    "del question_four_a['Assesor B']\n",
    "del question_four_a[\"2c Assessor B's Score (1-5)\"]\n",
    "del question_four_b['Assesor A']\n",
    "del question_four_b[\"2c Assesor A's Score (1-5)\"]\n",
    "\n",
    "question_four_a = question_four_a.rename(columns = {\"Assesor A\": \"Assessor\", \"2c Assesor A's Score (1-5)\": \"AssessorScore\"})\n",
    "question_four_b = question_four_b.rename(columns = {\"Assesor B\": \"Assessor\", \"2c Assessor B's Score (1-5)\": \"AssessorScore\"})\n",
    "question_four = pd.concat([question_four_a, question_four_b], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_four_scores = question_four.iloc[:, [2, 3, 4]].copy()\n",
    "logcount(q_four_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q_four_scores['LogCount'].values.reshape(-1, 1)\n",
    "Y = q_four_scores['AssessorScore'].values.reshape(-1, 1)\n",
    "regressquestion(q_four_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionstats(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,0], Y[:,0])\n",
    "\n",
    "predictedscore(question_four, slope, intercept)\n",
    "weightedscore(question_four)\n",
    "adjustedscore(question_four)\n",
    "assessordeviation(question_four, std_err)\n",
    "\n",
    "deviation_table['q2c'] = deviation_table['Assessor'].map(namedict(question_four))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue for Question 5 Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary transformations for table 5\n",
    "question_five_a = question_five.copy()\n",
    "question_five_b = question_five.copy()\n",
    "\n",
    "del question_five_a['Assesor B']\n",
    "del question_five_a[\"2f Assessor B's Score (1-5)\"]\n",
    "del question_five_b['Assesor A']\n",
    "del question_five_b[\"2f Assesor A's Score (1-5)\"]\n",
    "\n",
    "question_five_a = question_five_a.rename(columns = {\"Assesor A\": \"Assessor\", \"2f Assesor A's Score (1-5)\": \"AssessorScore\"})\n",
    "question_five_b = question_five_b.rename(columns = {\"Assesor B\": \"Assessor\", \"2f Assessor B's Score (1-5)\": \"AssessorScore\"})\n",
    "question_five = pd.concat([question_five_a, question_five_b], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_five_scores = question_five.iloc[:, [2, 3, 4]].copy()\n",
    "logcount(q_five_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q_five_scores['LogCount'].values.reshape(-1, 1)\n",
    "Y = q_five_scores['AssessorScore'].values.reshape(-1, 1)\n",
    "regressquestion(q_five_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,0], Y[:,0])\n",
    "\n",
    "predictedscore(question_five, slope, intercept)\n",
    "weightedscore(question_five)\n",
    "adjustedscore(question_five)\n",
    "assessordeviation(question_five, std_err)\n",
    "\n",
    "deviation_table['q2f'] = deviation_table['Assessor'].map(namedict(question_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue for Question 6 Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary transformations for table 6\n",
    "question_six_a = question_six.copy()\n",
    "question_six_b = question_six.copy()\n",
    "\n",
    "del question_six_a['Assesor B']\n",
    "del question_six_a[\"2g Assessor B's Score (1-5)\"]\n",
    "del question_six_b['Assesor A']\n",
    "del question_six_b[\"2g Assesor A's Score (1-5)\"]\n",
    "\n",
    "question_six_a = question_six_a.rename(columns = {\"Assesor A\": \"Assessor\", \"2g Assesor A's Score (1-5)\": \"AssessorScore\"})\n",
    "question_six_b = question_six_b.rename(columns = {\"Assesor B\": \"Assessor\", \"2g Assessor B's Score (1-5)\": \"AssessorScore\"})\n",
    "question_six = pd.concat([question_six_a, question_six_b], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_six_scores = question_six.iloc[:, [2, 3, 4]].copy()\n",
    "logcount(q_six_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = q_six_scores['LogCount'].values.reshape(-1, 1)\n",
    "Y = q_six_scores['AssessorScore'].values.reshape(-1, 1)\n",
    "regressquestion(q_six_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionstats(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,0], Y[:,0])\n",
    "\n",
    "predictedscore(question_six, slope, intercept)\n",
    "weightedscore(question_six)\n",
    "adjustedscore(question_six)\n",
    "assessordeviation(question_six, std_err)\n",
    "\n",
    "deviation_table['q2g'] = deviation_table['Assessor'].map(namedict(question_six))\n",
    "deviation_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_table.to_csv(r'C:\\Users\\User\\OneDrive\\Documents\\Student Life\\Protege Ventures\\Data Science\\Recruitment\\leniency.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
